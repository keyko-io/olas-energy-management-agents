{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Energy Consumption and Production Using Transformer Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "In this notebook, we will develop a machine learning model to predict whether the energy consumption of a household will be lower than its energy production in the next hour. We will use a Transformer model with the Time2Vector embedding to capture temporal information. The process includes data loading, preprocessing, normalization, PCA for dimensionality reduction, model training, and evaluation. Let's dive into the process step-by-step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "We start by importing all necessary libraries and modules required for our analysis and model building."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.saving import register_keras_serializable\n",
    "import time\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "import coloredlogs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Variables\n",
    "We define some global constants that will be used throughout the notebook, such as sequence length, batch size, learning rate, and other model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "SEQ_LENGTH = 60\n",
    "TRAIN_END_DATE = '2023-12-31'\n",
    "HEAD_SIZE = 256\n",
    "NUM_HEADS = 4\n",
    "FF_DIM = 4\n",
    "NUM_TRANSFORMER_BLOCKS = 4\n",
    "MLP_UNITS = [128]\n",
    "DROPOUT = 0.1\n",
    "MLP_DROPOUT = 0.1\n",
    "LEARNING_RATE = 1e-4\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 1\n",
    "EARLY_STOPPING_PATIENCE = 5\n",
    "\n",
    "DATA_DIR = 'data'\n",
    "MODELS_DIR = 'models'\n",
    "PROCESSED_DATA_DIR = 'processed_data'\n",
    "\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "os.makedirs(PROCESSED_DATA_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging Configuration\n",
    "We configure the logging to track the progress and debug information during the execution of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging configuration\n",
    "logger = logging.getLogger(__name__)\n",
    "coloredlogs.install(level='DEBUG', logger=logger, fmt='%(asctime)s %(levelname)s %(message)s')\n",
    "\n",
    "# Ignore TF warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "tf.get_logger().setLevel('ERROR')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow and Device Configuration\n",
    "We configure TensorFlow to use GPUs if available for faster computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-29 14:00:59 INFO 1 physical GPUs, 1 logical GPUs\n"
     ]
    }
   ],
   "source": [
    "# Forcing GPU usage\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        logger.info(f\"{len(gpus)} physical GPUs, {len(logical_gpus)} logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        logger.error(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Definitions\n",
    "We define all the functions required for data loading, preprocessing, PCA application, dataset creation, model building, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_data(file_path):\n",
    "    logger.info(\"Loading data\")\n",
    "    return pd.read_csv(file_path, parse_dates=['cet_cest_timestamp'], index_col='cet_cest_timestamp')\n",
    "\n",
    "def preprocess_data(data):\n",
    "    logger.info(\"Preparing data\")\n",
    "    data['residential2_circulation_pump'] = data['DE_KN_residential2_circulation_pump'].diff().fillna(0)\n",
    "    data['residential2_dishwasher'] = data['DE_KN_residential2_dishwasher'].diff().fillna(0)\n",
    "    data['residential2_freezer'] = data['DE_KN_residential2_freezer'].diff().fillna(0)\n",
    "    data['residential2_washing_machine'] = data['DE_KN_residential2_washing_machine'].diff().fillna(0)\n",
    "    data['total_consumption'] = data['DE_KN_residential2_grid_import'].diff().fillna(0)\n",
    "    data['total_production'] = data['DE_KN_residential1_pv']\n",
    "    data = data.ffill()\n",
    "    data['air_conditioning_on'] = 0\n",
    "    data.loc[data['total_production'] > data['total_consumption'], 'air_conditioning_on'] = 1\n",
    "    data['air_conditioning_on'] = data['air_conditioning_on'].shift(1, fill_value=0)\n",
    "    X_minutes = 60\n",
    "    data['target'] = data['air_conditioning_on'].rolling(window=X_minutes).sum().shift(-X_minutes) > X_minutes * 0.5\n",
    "    data.dropna(subset=['target'], inplace=True)\n",
    "    data['target'] = data['target'].astype(int)  # Convert target to integer\n",
    "    return data\n",
    "\n",
    "def select_features(data):\n",
    "    logger.info(\"Selecting features\")\n",
    "    features = [\n",
    "        'DE_KN_residential2_circulation_pump', \n",
    "        'DE_KN_residential2_dishwasher', \n",
    "        'DE_KN_residential2_freezer', \n",
    "        'DE_KN_residential2_grid_import',\n",
    "        'DE_KN_residential2_washing_machine',\n",
    "        'DE_KN_residential1_pv'\n",
    "    ]\n",
    "    return data[features], data['target']\n",
    "\n",
    "def normalize_data(X_train, X_valid, X_test):\n",
    "    logger.info(\"Normalizing data\")\n",
    "    scaler_path = os.path.join(MODELS_DIR, 'scaler.pkl')\n",
    "    if os.path.exists(scaler_path):\n",
    "        scaler = joblib.load(scaler_path)\n",
    "        logger.info(f\"Loading scaler from {scaler_path}\")\n",
    "    else:\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(X_train)\n",
    "        joblib.dump(scaler, scaler_path)\n",
    "        logger.info(f\"Saving scaler to {scaler_path}\")\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_valid = scaler.transform(X_valid)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    return X_train, X_valid, X_test\n",
    "\n",
    "def apply_pca(X_train, X_valid, X_test):\n",
    "    logger.info(\"Applying PCA\")\n",
    "    pca_path = os.path.join(MODELS_DIR, 'pca.pkl')\n",
    "    if os.path.exists(pca_path):\n",
    "        pca = joblib.load(pca_path)\n",
    "        logger.info(f\"PCA model loaded from {pca_path}\")\n",
    "    else:\n",
    "        pca = PCA(n_components=0.95)  # Retain 95% of variance\n",
    "        pca.fit(X_train)\n",
    "        joblib.dump(pca, pca_path)\n",
    "        logger.info(f\"Saved PCA model to {pca_path}\")\n",
    "    X_train = pca.transform(X_train)\n",
    "    X_valid = pca.transform(X_valid)\n",
    "    X_test = pca.transform(X_test)\n",
    "    return X_train, X_valid, X_test\n",
    "\n",
    "def remove_nans(X, y):\n",
    "    logger.info(\"Deleting rows with NaN values\")\n",
    "    mask = ~np.isnan(X).any(axis=1)\n",
    "    X = X[mask]\n",
    "    y = y[mask]\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def create_tf_dataset(X, y, seq_length, batch_size, dataset_name):\n",
    "    logger.info(f\"Creating dataset {dataset_name}\")\n",
    "    X_data, y_data = [], []\n",
    "    for i in range(len(X) - seq_length - 1):\n",
    "        X_data.append(X[i:(i + seq_length)])\n",
    "        y_data.append(y[i + seq_length])\n",
    "    X_data, y_data = np.array(X_data), np.array(y_data)\n",
    "    with tf.device('/gpu:0'):\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((X_data, y_data))\n",
    "        dataset = dataset.cache().shuffle(buffer_size=len(X_data)).batch(batch_size).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "@register_keras_serializable()\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, **kwargs):\n",
    "        super(TransformerBlock, self).__init__(**kwargs)\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = tf.keras.Sequential([layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim)])\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        final_output = self.layernorm2(out1 + ffn_output)\n",
    "        return final_output\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(TransformerBlock, self).build(input_shape)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(TransformerBlock, self).get_config()\n",
    "        config.update({\n",
    "            'embed_dim': self.att.key_dim,\n",
    "            'num_heads': self.att.num_heads,\n",
    "            'ff_dim': self.ffn.layers[0].units,\n",
    "            'rate': self.dropout1.rate,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "\n",
    "@register_keras_serializable()\n",
    "class Time2Vector(layers.Layer):\n",
    "    def __init__(self, seq_len, **kwargs):\n",
    "        super(Time2Vector, self).__init__(**kwargs)\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.weights_linear = self.add_weight(name='weight_linear', shape=(int(self.seq_len),), initializer='uniform', trainable=True)\n",
    "        self.bias_linear = self.add_weight(name='bias_linear', shape=(int(self.seq_len),), initializer='uniform', trainable=True)\n",
    "        self.weights_periodic = self.add_weight(name='weight_periodic', shape=(int(self.seq_len),), initializer='uniform', trainable=True)\n",
    "        self.bias_periodic = self.add_weight(name='bias_periodic', shape=(int(self.seq_len),), initializer='uniform', trainable=True)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = tf.math.reduce_mean(x[:,:,:4], axis=-1)\n",
    "        time_linear = self.weights_linear * x + self.bias_linear\n",
    "        time_linear = tf.expand_dims(time_linear, axis=-1)\n",
    "        time_periodic = tf.math.sin(tf.multiply(x, self.weights_periodic) + self.bias_periodic)\n",
    "        time_periodic = tf.expand_dims(time_periodic, axis=-1)\n",
    "        return tf.concat([time_linear, time_periodic], axis=-1)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(Time2Vector, self).get_config()\n",
    "        config.update({'seq_len': self.seq_len})\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "\n",
    "def build_model(input_shape, head_size, num_heads, ff_dim, num_transformer_blocks, mlp_units, dropout, mlp_dropout):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    time_embedding = Time2Vector(SEQ_LENGTH)(inputs)\n",
    "    x = layers.Concatenate(axis=-1)([inputs, time_embedding])\n",
    "    x = layers.Dense(head_size, dtype='float32')(x)\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        x = TransformerBlock(head_size, num_heads, ff_dim, dropout)(x, training=True)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    for dim in mlp_units:\n",
    "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(mlp_dropout)(x)\n",
    "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "def plot_loss(history):\n",
    "    logger.info(\"Plotting training and validation loss\")\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "We load the household energy consumption data from a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-29 14:00:59 INFO Loading data\n",
      "/var/folders/gd/q32w8wgd1bxc7wq53vd9nts80000gn/T/ipykernel_940/992125112.py:3: DtypeWarning: Columns (70) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  return pd.read_csv(file_path, parse_dates=['cet_cest_timestamp'], index_col='cet_cest_timestamp')\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "data = load_data(os.path.join(DATA_DIR, 'household_data_1min_singleindex.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "We preprocess the data by creating new features, handling missing values, and defining the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-29 14:01:27 INFO Preparing data\n",
      "2024-07-29 14:01:32 INFO Selecting features\n"
     ]
    }
   ],
   "source": [
    "# Preprocess data\n",
    "data_preprocessed = preprocess_data(data)\n",
    "X, y = select_features(data_preprocessed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data into Training, Validation, and Test Sets\n",
    "We split the data into training, validation, and test sets to prepare for model training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide data into train, validation and test sets\n",
    "split_1 = int(len(X) * 0.7)\n",
    "split_2 = int(len(X) * 0.85)\n",
    "X_train, X_valid, X_test = X[:split_1], X[split_1:split_2], X[split_2:]\n",
    "y_train, y_valid, y_test = y[:split_1], y[split_1:split_2], y[split_2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove NaNs\n",
    "We remove any rows containing NaN values from the datasets before applying PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-29 14:01:32 INFO Deleting rows with NaN values\n",
      "2024-07-29 14:01:32 INFO Deleting rows with NaN values\n",
      "2024-07-29 14:01:32 INFO Deleting rows with NaN values\n"
     ]
    }
   ],
   "source": [
    "# Delete rows with NaN values before normalization\n",
    "X_train, y_train = remove_nans(X_train, y_train)\n",
    "X_valid, y_valid = remove_nans(X_valid, y_valid)\n",
    "X_test, y_test = remove_nans(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Normalization\n",
    "We normalize the data to ensure that all features have the same scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-29 14:01:33 INFO Normalizing data\n",
      "2024-07-29 14:01:33 INFO Loading scaler from models/scaler.pkl\n"
     ]
    }
   ],
   "source": [
    "# Normalize data\n",
    "X_train_norm, X_valid_norm, X_test_norm = normalize_data(X_train, X_valid, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply PCA\n",
    "We apply PCA to reduce the dimensionality of the data, retaining 95% of the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-29 14:01:33 INFO Applying PCA\n",
      "2024-07-29 14:01:33 INFO PCA model loaded from models/pca.pkl\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Apply PCA\n",
    "X_train_pca, X_valid_pca, X_test_pca = apply_pca(X_train_norm, X_valid_norm, X_test_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create TensorFlow Datasets\n",
    "We create TensorFlow datasets from the processed and normalized data, which will be used for training and evaluating the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-29 14:01:33 INFO Creating dataset train\n",
      "/var/folders/gd/q32w8wgd1bxc7wq53vd9nts80000gn/T/ipykernel_940/992125112.py:80: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  y_data.append(y[i + seq_length])\n",
      "2024-07-29 14:01:41 INFO Creating dataset valid\n",
      "2024-07-29 14:01:42 INFO Creating dataset test\n"
     ]
    }
   ],
   "source": [
    "# Create datasets\n",
    "train_dataset = create_tf_dataset(X_train_pca, y_train, SEQ_LENGTH, BATCH_SIZE, 'train')\n",
    "valid_dataset = create_tf_dataset(X_valid_pca, y_valid, SEQ_LENGTH, BATCH_SIZE, 'valid')\n",
    "test_dataset = create_tf_dataset(X_test_pca, y_test, SEQ_LENGTH, BATCH_SIZE, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "We build, compile, and train the Transformer model using the training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-29 14:01:44 INFO Loading existing model\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_19\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_19\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_15      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ time2_vector_3      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">240</span> │ input_layer_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Time2Vector</span>)       │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_3       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ time2_vector_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_33 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280</span> │ concatenate_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ transformer_block_… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,055,236</span> │ dense_33[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)  │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ transformer_block_… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,055,236</span> │ transformer_bloc… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)  │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ transformer_block_… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,055,236</span> │ transformer_bloc… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)  │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ transformer_block_… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,055,236</span> │ transformer_bloc… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)  │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ transformer_bloc… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_42 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │ global_average_p… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_51          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_42[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_43 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │ dropout_51[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_15      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m2\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ time2_vector_3      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m2\u001b[0m)     │        \u001b[38;5;34m240\u001b[0m │ input_layer_15[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mTime2Vector\u001b[0m)       │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_3       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m4\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ input_layer_15[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ time2_vector_3[\u001b[38;5;34m0\u001b[0m… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_33 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │      \u001b[38;5;34m1,280\u001b[0m │ concatenate_3[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ transformer_block_… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │  \u001b[38;5;34m1,055,236\u001b[0m │ dense_33[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)  │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ transformer_block_… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │  \u001b[38;5;34m1,055,236\u001b[0m │ transformer_bloc… │\n",
       "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)  │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ transformer_block_… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │  \u001b[38;5;34m1,055,236\u001b[0m │ transformer_bloc… │\n",
       "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)  │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ transformer_block_… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │  \u001b[38;5;34m1,055,236\u001b[0m │ transformer_bloc… │\n",
       "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)  │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ transformer_bloc… │\n",
       "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_42 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m32,896\u001b[0m │ global_average_p… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_51          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_42[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_43 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │        \u001b[38;5;34m129\u001b[0m │ dropout_51[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,255,489</span> (16.23 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,255,489\u001b[0m (16.23 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,255,489</span> (16.23 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,255,489\u001b[0m (16.23 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m43226/43226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5764s\u001b[0m 133ms/step - accuracy: 1.0000 - loss: 7.6679e-11 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-29 15:37:49 INFO Training time: 5763.91 seconds\n",
      "2024-07-29 15:37:50 INFO Model saved to models/transformer_model.keras\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_path = os.path.join(MODELS_DIR, 'transformer_model.keras')\n",
    "if os.path.exists(model_path):\n",
    "    logger.info(\"Loading existing model\")\n",
    "    model = tf.keras.models.load_model(model_path, custom_objects={\"Time2Vector\": Time2Vector, \"TransformerBlock\": TransformerBlock})\n",
    "else:\n",
    "    input_shape = (SEQ_LENGTH, X_train_pca.shape[1])\n",
    "    logger.info(\"Building new model\")\n",
    "    model = build_model(input_shape, HEAD_SIZE, NUM_HEADS, FF_DIM, NUM_TRANSFORMER_BLOCKS, MLP_UNITS, DROPOUT, MLP_DROPOUT)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=EARLY_STOPPING_PATIENCE, restore_best_weights=True)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "history = model.fit(train_dataset, validation_data=valid_dataset, epochs=EPOCHS, callbacks=[early_stopping], verbose=1)\n",
    "\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "logger.info(f\"Training time: {training_time:.2f} seconds\")\n",
    "\n",
    "model.save(model_path)\n",
    "logger.info(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "We evaluate the model using the test dataset and visualize the training and validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-29 15:37:50 INFO Evaluation on test set\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10813/10813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m182s\u001b[0m 17ms/step - accuracy: 0.9998 - loss: 0.0044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-29 15:40:52 INFO Loss on test: 0.004827520810067654\n",
      "2024-07-29 15:40:52 INFO Accuracy on test: 0.9998294711112976\n"
     ]
    }
   ],
   "source": [
    "\n",
    "logger.info(\"Evaluation on test set\")\n",
    "test_loss, test_accuracy = model.evaluate(test_dataset)\n",
    "logger.info(f\"Loss on test: {test_loss}\")\n",
    "logger.info(f\"Accuracy on test: {test_accuracy}\")\n",
    "\n",
    "#plot_loss(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
